{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exuyRTG-WDw0"
   },
   "source": [
    "## Programming assignment. Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is graded by your `submission.json` which is created by a cell below. To generate your submission file you need to fill the values of **GRADING_ANSWER** instance fields.\n",
    "\n",
    "You can press **\"Submit Assignment\"** at any time to submit partial progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from submit import Answer\n",
    "\n",
    "GRADING_ANSWER = Answer()\n",
    "GRADING_ANSWER.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 3 covers count-based and neural-based language models, smoothing methods, simple text generation with greed search, methods for evaluation of language models, and sequence labelling tasks.\n",
    "\n",
    "<br>\n",
    "\n",
    "The objective of this programming assignment is to consolidate your knowledge on language models through practice. We thus will go through some of the aspects that we covered in the videos and implement them to better understand the inner workings of the models and methods.\n",
    "\n",
    "<br>\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SWmHpnOTo8m"
   },
   "source": [
    "#### Let us use the Gutenberg corpus which contains publicly available electronic books. Let us download the corpus using the NLTK library and have a look at the texts we can use to train our language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0iKEVKCI6d9",
    "outputId": "6927d5c3-0b74-4dd0-e0c5-71606c437509"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "fileids = gutenberg.fileids()\n",
    "fileids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D37q86CoYAHH"
   },
   "source": [
    "#### As you know, the first step is data preprocessing that depends on the data, task and the models we use: cleaning data, sentence segmentation, tokenization, etc. For simplicity, let us use the NLTK built-in functions that allow to load segmented and tokenized sentences from the corpus. In this notebook, we will use the sentences from \"Alice in Wonderland\" by Lewis Carroll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LtBUjgTYPQb",
    "outputId": "0ab698b5-4b4c-4651-e8c3-200d819ea073"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  'Alice',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'Adventures',\n",
       "  'in',\n",
       "  'Wonderland',\n",
       "  'by',\n",
       "  'Lewis',\n",
       "  'Carroll',\n",
       "  '1865',\n",
       "  ']'],\n",
       " ['CHAPTER', 'I', '.']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_sents = gutenberg.sents(fileids[7])\n",
    "alice_sents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAzSr-9pZ9aU"
   },
   "source": [
    "### Task 0. (1 point)\n",
    "##### Write a function that lowers the input tokens in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "9yxigkGxaJJv"
   },
   "outputs": [],
   "source": [
    "def lower_sents(sents: list) -> list:\n",
    "    \"\"\"\n",
    "    :param sents: a list of tokenized sentences from the NLTK corpora\n",
    "    :returns: a list of lowered tokenized sentences\n",
    "    \"\"\"\n",
    "    res = [[t.lower() for t in s] for s in sents]\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "w-5l38K7ayAy"
   },
   "outputs": [],
   "source": [
    "test_idx1 = 2\n",
    "test_idx2 = 143\n",
    "\n",
    "## here is the sentences lowered with your function\n",
    "lower_alice_sents = lower_sents(alice_sents)\n",
    "\n",
    "assert lower_alice_sents[test_idx1] == ['down', 'the', 'rabbit', '-', 'hole']\n",
    "assert lower_alice_sents[test_idx2] == ['ever', 'so', 'many', 'lessons', 'to', 'learn', '!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "nnAPlLWjXY1B"
   },
   "outputs": [],
   "source": [
    "TASK_INDEXES = [25, 40]\n",
    "\n",
    "ANSWER = [\n",
    "    lower_alice_sents[i] for i in [25, 40]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "rYEmZ6ZiXtsb"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "def prepare_answer_for_submitting(answer):\n",
    "    return '|'.join(answer[0]) + '|'.join(answer[1])\n",
    "GRADING_ANSWER.Q1 = prepare_answer_for_submitting(ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnWPCT9lh3vb"
   },
   "source": [
    "### Recap: N-Gram language model \n",
    "\n",
    "Let us recap the definition of a language model and what tasks can be solved with the help of them. First, a language model is a probabilistic model that can estimate the sentence probability and predict the next element in the sequence, e.g. the next symbol or the next word.\n",
    "\n",
    "The probability $P$ of the sentence $S$ is framed as the joint probability of all words in the sentence:\n",
    "$P(S) = P(w_1, \\dots, w_N)$, where $N$ is the number of words in the sentence.\n",
    "\n",
    "**Chain rule** is used to compute the sentence probability based on the probability of the current word conditioned on the preceding ones:\n",
    "$$P(w_1, \\dots, w_N) = P(w_1) P(w_2 \\mid w_1) \\dots P(w_N \\mid w_{1:N-1}).$$ \n",
    "\n",
    "As you remember from the videos, this is not common in practice since the sentence probability can be zeroed out.\n",
    "\n",
    "To this end, the Markov property was introduced. It suggests that the next word in an N-gram language model depends on a fixed number of the preceding words:\n",
    "\n",
    "$$P(w_i \\mid w_1, \\dots, w_{i - 1}) = P(w_i \\mid w_{i - N + 1}, \\dots, w_{i - 1})$$\n",
    "\n",
    "* $N = 1: P(w_i \\mid w_1, \\dots, w_{i - 1}) = P(w_i)$\n",
    "* $N = 2: P(w_i \\mid w_1, \\dots, w_{i - 1}) = P(w_i \\mid w_{i - 1})$\n",
    "* $N = 3: P(w_i \\mid w_1, \\dots, w_{i - 1}) = P(w_i \\mid w_{i - 2}, w_{i - 1})$\n",
    "\n",
    "The sentence probability is then the product of the word probabilities conditioned on a specified number of the preceding words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nBjeULTp_JP"
   },
   "source": [
    "## Task 1: Data pre-processing\n",
    "##### For this task, you need to implement two functions that help to pre-process the data for training an N-gram language model. There are two aspects that should be considered with respect to the data pre-processing format:\n",
    " \n",
    "1.   If the context is less than ${(N - 1)}$ tokens, the sample should be padded on the left with the special token $<s>$ that denotes the beginning of the sequence. Consider an example of the pre-processed data for sentence \"*We study NLP*\":\n",
    "\n",
    "$N = 2$\n",
    "\n",
    "*   $[<s>, We]$\n",
    "*   $[We, study]$\n",
    "*   $[study, NLP]$\n",
    "\n",
    "$N = 3$\n",
    "\n",
    "*   $[<s>, <s>, We]$\n",
    "*   $[<s>, We, study]$\n",
    "*   $[We, study, NLP]$\n",
    "\n",
    "\n",
    "2.  Before splitting each sample into N-grams, you need to add the special token $</s>$ at the end of each sequence for the model to understand the step it can finish the generation procedure. For instance:\n",
    "\n",
    "*   $[We, study, NLP, </s>]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fy8crfqUIytt"
   },
   "source": [
    "#### Task 1.1 (2 points)\n",
    "\n",
    "The first step is to pre-process the tokenized sentences by adding the special tokens as described above. You can use the ```ngrams``` function from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "X1B-ku_aFLNd"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "\n",
    "def generate_ngrams(tokenized_sentences: list, n: int, bos_token: str = \"<s>\", eos_token: str = \"</s>\") -> list:\n",
    "    \"\"\"\n",
    "    :param tokenized_sentences: list of tokenized sentences\n",
    "    :param n: the size of the N-gram\n",
    "    :param bos_token: the special token that denotes the beginning of the sentence\n",
    "    :param eos_token: the special token that denotes the end of the sentence\n",
    "    :returns: a list of generated N-grams\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    \n",
    "    for s in tokenized_sentences:\n",
    "        modified_s = [bos_token for i in range(n-1)] + s + [eos_token]\n",
    "        \n",
    "        for i in range(0, len(modified_s)-n+1):\n",
    "            res.append(modified_s[i:i+n])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJPaHSGbMy3A"
   },
   "source": [
    "Apply the implemented function ```generate_ngrams``` to generate samples for $N = 2$ and for $N = 3$ using the pre-processed sentences ```lower_alice_sents```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "IyLdCfrOJR8y"
   },
   "outputs": [],
   "source": [
    "# N = 2\n",
    "alice_bigrams = generate_ngrams(lower_alice_sents, n=2)\n",
    "\n",
    "assert alice_bigrams[test_idx1] == ['alice', \"'\"]\n",
    "assert alice_bigrams[test_idx2] == ['rabbit', 'with']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "5opVlX3LJta2"
   },
   "outputs": [],
   "source": [
    "# N = 3\n",
    "alice_trigrams = generate_ngrams(lower_alice_sents, n=3)\n",
    "\n",
    "assert alice_trigrams[test_idx1] == ['[', 'alice', \"'\"]\n",
    "assert alice_trigrams[test_idx2] == ['white', 'rabbit', 'with']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "OUE1OMOOYMFu"
   },
   "outputs": [],
   "source": [
    "TASK_INDEXES = [15, 35]\n",
    "\n",
    "ANSWER_BIGRAM = [\n",
    "    alice_bigrams[i] for i in TASK_INDEXES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-bcuawC0Yf8F",
    "outputId": "92f93157-8ecb-4f3a-f83c-81702ff2bdb5"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "GRADING_ANSWER.Q2 = prepare_answer_for_submitting(ANSWER_BIGRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "dT1xZPy2YZFb"
   },
   "outputs": [],
   "source": [
    "ANSWER_TRIGRAM = [\n",
    "    alice_trigrams[i] for i in TASK_INDEXES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "RzkP3QwBYVS-"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "GRADING_ANSWER.Q3 = prepare_answer_for_submitting(ANSWER_TRIGRAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yIL6xaqLvMO"
   },
   "source": [
    "#### Task 1.2 (2 point)\n",
    "\n",
    "The second step is to write a function that helps to count how many times each word in the corpus appeared after ${(N - 1)}$ preceding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', '['],\n",
       " ['[', 'alice'],\n",
       " ['alice', \"'\"],\n",
       " [\"'\", 's'],\n",
       " ['s', 'adventures'],\n",
       " ['adventures', 'in'],\n",
       " ['in', 'wonderland'],\n",
       " ['wonderland', 'by'],\n",
       " ['by', 'lewis'],\n",
       " ['lewis', 'carroll']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ngrams(lower_alice_sents, n=2)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "0H-JAytyNWLh"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "\n",
    "def count_ngrams(ngrams: list, n: int) -> defaultdict:\n",
    "    \"\"\"\n",
    "    :param ngrams: a list of ngrams generated at the previous step\n",
    "    :param n: the size of the N-gram (either 2 or 3 in our case)\n",
    "    :returns ngram_counts: a nested dictionary which stores counters for the contexts\n",
    "    of (N - 1) size and the words that follow these contexts, e.g.:\n",
    "    * N = 2\n",
    "    bigram_d = {\n",
    "        ...,\n",
    "        'study': {\n",
    "            'NLP': 1\n",
    "        }\n",
    "    }\n",
    "    * N = 3\n",
    "    trigram_d = {\n",
    "        ...,\n",
    "        'We study': {\n",
    "            'NLP': 1\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    ngram_counts = defaultdict(Counter)\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        \n",
    "        k = ngram[0:n-1]\n",
    "        sub_k = ngram[n-1]\n",
    "        \n",
    "        if len(k) > 1: \n",
    "            k = tuple(k)\n",
    "        else:\n",
    "            k = k[0]\n",
    "\n",
    "        ngram_counts[k][sub_k] += 1\n",
    "        \n",
    "    return ngram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ERd_qf3Z1Cv",
    "outputId": "8f26582d-7c33-4fa1-f3d2-cb6667c9cdc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'.': 1,\n",
       "         'wouldn': 1,\n",
       "         'argue': 1,\n",
       "         'hid': 1,\n",
       "         ',': 1,\n",
       "         'got': 1,\n",
       "         'order': 1,\n",
       "         \",'\": 2,\n",
       "         'of': 1})"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_d = count_ngrams(alice_bigrams, n=2)\n",
    "\n",
    "## look at the example of the bigram_d key-value pair\n",
    "bigram_d[\"creatures\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "xWm3NIsXp165"
   },
   "outputs": [],
   "source": [
    "assert bigram_d[\"creatures\"][\"argue\"] == 1\n",
    "assert bigram_d[\"called\"][\"out\"] == 7\n",
    "assert len(bigram_d[\"always\"]) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2ufUhFEaRjh",
    "outputId": "eb1a4eea-190f-4f52-9662-b14571082cdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'herself': 5, 'her': 2})"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_d = count_ngrams(alice_trigrams, n=3)\n",
    "\n",
    "## look at the example of the trigram_d key-value pair\n",
    "trigram_d[\"saying\", \"to\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "1gwYB8Sxp6w8"
   },
   "outputs": [],
   "source": [
    "assert trigram_d[\"and\", \"the\"][\"moral\"] == 5\n",
    "assert trigram_d[\"saying\", \"to\"][\"herself\"] == 5\n",
    "assert len(trigram_d[\"like\", \"the\"]) == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "iUYbBYMjYySW"
   },
   "outputs": [],
   "source": [
    "TASK_WORDS = [\"moral\", \"queen\"]\n",
    "\n",
    "ANSWER_BIGRAM_D = [\n",
    "    len(bigram_d[word]) for word in TASK_WORDS\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "QTh8iAHmY_Pa"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "GRADING_ANSWER.Q4 = str(ANSWER_BIGRAM_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyILDExqebhT"
   },
   "source": [
    "## Task 2: N-gram language model\n",
    "\n",
    "#### Task 2.1 (1 point)\n",
    "In the previous task, we learned how to implement simple count-based statistics for arbitrary N-grams. We now need to use the statistics to compute the probabilities of the next word given the preceding $(N - 1)$ words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "id": "Mi7pn8D7O6uH"
   },
   "outputs": [],
   "source": [
    "def compute_probabilities(ngram_d: defaultdict) -> defaultdict:\n",
    "    \"\"\"\n",
    "    :param ngram_d: a nested dictionary which stores counters for the contexts\n",
    "    of (N - 1) size and the words that follow these contexts\n",
    "    :param n: the size of the N-gram\n",
    "    :returns: a nested dictionary \n",
    "    \"\"\"\n",
    "    probabilities_d = defaultdict(Counter)\n",
    "    \n",
    "    for n_gram, n_counter in ngram_d.items():\n",
    "        total = sum(n_counter.values())\n",
    "        for k, v in n_counter.items():\n",
    "            probabilities_d[n_gram][k] = v / total\n",
    "        \n",
    "    return probabilities_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i7JA0d25mlhd",
    "outputId": "4030dc73-5c72-457d-a0ea-3f9089ed390b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'.': 0.1,\n",
       "         'wouldn': 0.1,\n",
       "         'argue': 0.1,\n",
       "         'hid': 0.1,\n",
       "         ',': 0.1,\n",
       "         'got': 0.1,\n",
       "         'order': 0.1,\n",
       "         \",'\": 0.2,\n",
       "         'of': 0.1})"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_probabilities = compute_probabilities(bigram_d)\n",
    "\n",
    "## let us have a look at the probabilities for the tokens that are probably to follow the word \"creatures\"\n",
    "bigram_probabilities[\"creatures\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "id": "hDZrrFxEqPAs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "assert np.allclose(sum(bigram_probabilities[\"creatures\"].values()), 1)\n",
    "assert round(bigram_probabilities[\"called\"][\"out\"], 2) == 0.47\n",
    "assert len(bigram_probabilities[\"always\"]) == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_1TnP3QryLD",
    "outputId": "31265228-34d4-4afc-a755-2c8ee732a202"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'herself': 0.7142857142857143, 'her': 0.2857142857142857})"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_probabilities = compute_probabilities(trigram_d)\n",
    "\n",
    "## let us have a look at the probabilities for the tokens that are probably to follow the bigram \"saying to\"\n",
    "trigram_probabilities[\"saying\", \"to\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "5oG86Y4wKsH0"
   },
   "outputs": [],
   "source": [
    "assert np.allclose(sum(trigram_probabilities[\"and\", \"the\"].values()), 1)\n",
    "assert round(trigram_probabilities[\"saying\", \"to\"][\"herself\"], 2) == 0.71\n",
    "assert len(trigram_probabilities[\"like\", \"the\"]) == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "id": "vjw2mfVoZZmf"
   },
   "outputs": [],
   "source": [
    "ANSWER = list(bigram_probabilities[\"always\"].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "UHvcSzGJZp-a"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "GRADING_ANSWER.Q5 = sum(ANSWER) / len(ANSWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRM4UIbLtxrX"
   },
   "source": [
    "#### Task 2.2 (2 point)\n",
    "Once we have implemented all the functions we need to construct our count-based language model, let us then do this :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "_AGNu8RnvI-H"
   },
   "outputs": [],
   "source": [
    "class NgramLM(object):\n",
    "    def __init__(self, sentences: list, n: int, bos_token: str = \"<s>\", eos_token: str = \"</s>\"):\n",
    "        assert n in [2, 3]\n",
    "        self.n = n\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.model_probabilities = self.get_model_probabilities(\n",
    "            sentences=sentences,\n",
    "            n=self.n,\n",
    "            bos_token=self.bos_token,\n",
    "            eos_token=self.eos_token\n",
    "        )\n",
    "        \n",
    "    def get_model_probabilities(self, sentences: list, n: int, bos_token: str, eos_token: str) -> defaultdict:\n",
    "        \n",
    "        ## use your function from task 1.1 to generate ngrams from the input sentences\n",
    "        n_grams = generate_ngrams(sentences, n, bos_token, eos_token)\n",
    "        \n",
    "        ## use your function from task 1.2 to compute statistics for each resulted N-gram\n",
    "        n_grams = count_ngrams(n_grams, n)\n",
    "        \n",
    "        ## use your function from task 2.1 to compute probabilities from the statistics you computed at the previous step\n",
    "        model_probabilities = compute_probabilities(n_grams)\n",
    "        \n",
    "        \n",
    "        return model_probabilities\n",
    "\n",
    "    def get_context_hypotheses(self, context: str) -> defaultdict:\n",
    "        context = context.split()\n",
    "        context = context[max(0, len(context) - self.n + 1):]\n",
    "        context = tuple([self.bos_token] * (self.n - 1 - len(context)) + context)\n",
    "        if self.n == 2:\n",
    "            context = context[0]\n",
    "        hypotheses = self.model_probabilities.get(context, 0)\n",
    "        return hypotheses\n",
    "\n",
    "    def get_next_word_probability(self, context: str, next_word: str) -> float:\n",
    "        context_hypotheses = self.get_context_hypotheses(context)\n",
    "        if context_hypotheses:\n",
    "            next_word_probability = context_hypotheses.get(next_word, 0)\n",
    "            return next_word_probability\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ5PUCCs0450"
   },
   "source": [
    "#### $N = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "Ndoan2aK0-2O"
   },
   "outputs": [],
   "source": [
    "bigram_model = NgramLM(lower_alice_sents, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "id": "mtd9dD4t0B_p"
   },
   "outputs": [],
   "source": [
    "## let us have a look at some examples of the bigram language model\n",
    "assert len(bigram_model.get_context_hypotheses(\"alice\")) == 97\n",
    "assert np.allclose(round(bigram_model.get_context_hypotheses(\"dog\")['growls'], 2), 0.33)\n",
    "assert np.allclose(round(bigram_model.get_next_word_probability(\"<s>\", \"alice\"), 2), 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdxxaGbs3UwE"
   },
   "source": [
    "## $N = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "id": "pOS2nHoD3WnZ"
   },
   "outputs": [],
   "source": [
    "trigram_model = NgramLM(lower_alice_sents, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "dXSu7XdB3rxc"
   },
   "outputs": [],
   "source": [
    "## let us have a look at some examples of the trigram language model\n",
    "assert len(trigram_model.get_context_hypotheses(\"<s> <s>\")) == 167\n",
    "assert np.allclose(round(trigram_model.get_next_word_probability(\"<s> alice\", \"whispered\"), 2), 0.01)\n",
    "assert np.allclose(trigram_model.get_next_word_probability(\"<s> a\", \"queen\"), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Id2yPmM86ZBV"
   },
   "source": [
    "### Task 3: Text generation (2 points)\n",
    "\n",
    "Let us implement a decoding strategy to generate texts with our N-gram language models. For this, we will use the greed search method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "lNnlNWXC8Jub"
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def greed_search(model: NgramLM, context: str):\n",
    "    hypotheses = model.get_context_hypotheses(context)\n",
    "    \n",
    "    return max(hypotheses, key=hypotheses.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "id": "fznm_GhE9s_k"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "GRADING_ANSWER.Q6 = greed_search(bigram_model, \"queen\")\n",
    "GRADING_ANSWER.Q7 = greed_search(trigram_model, \"<s> alice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqWUmvzv-jRF"
   },
   "source": [
    "### Let us see a few examples of text generation for both bigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "id": "gBVe_7qj-mTL"
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "context = \"<s>\"\n",
    "\n",
    "for step in range(k):\n",
    "    next_word = greed_search(bigram_model, context)\n",
    "    context += f\" {next_word}\"\n",
    "    if context.endswith(\"</s>\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "dCtvVdlY--O5",
    "outputId": "827a40d6-7ce6-4389-e06f-671c5ba7c259"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> ' t be a little thing !' </s>\""
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "FVoIIElF_VyB"
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "context = \"<s> alice\"\n",
    "\n",
    "for step in range(k):\n",
    "    next_word = greed_search(trigram_model, context)\n",
    "    context += f\" {next_word}\"\n",
    "    if context.endswith(\"</s>\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Bpl7tWKiaTrg",
    "outputId": "5128c08a-f472-4a1c-9d1a-7c3f40d0dd8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> alice was not a moment to be a great hurry .'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRCQ7BW-DRKy"
   },
   "source": [
    "### Task 4: Language model perplexity (2 points)\n",
    "\n",
    "As of now, we hope you got understanding of the inner workings of the count-based language model. So how can we evaluate it using perplexity?\n",
    "\n",
    "For simplicity, let us use an NLTK implementation of the count-based language model that is referred to as the maximum log-likelihood estimation (MLE). We will also have a look at the ready-to-go functions that help us to pre-process our data for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q34QVuBHc4fJ"
   },
   "source": [
    "We will use the ```padded_everygram_pipeline``` function that allows for pre-processing the data in the way you did this in the first task.\n",
    "\n",
    "*   ```data``` is an iterator with the pre-processed N-grams\n",
    "*   ```chained_data``` an iterator with chained sentences for a flat stream of words in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "UZWkoRlxbILi"
   },
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "\n",
    "n = 2\n",
    "idx = 1500\n",
    "train_sents, test_sents = lower_alice_sents[:idx], lower_alice_sents[idx:]\n",
    "train_data, train_chains = padded_everygram_pipeline(n, train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3aLYL2Kea72b",
    "outputId": "99e89d77-07e8-41db-c497-85d98bc47430"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2473"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "\n",
    "\n",
    "bigram_model = MLE(n)\n",
    "bigram_model.fit(train_data, train_chains)\n",
    "len(bigram_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "id": "vhqBocxYfAPI"
   },
   "outputs": [],
   "source": [
    "assert bigram_model.counts[\"called\"] == 15\n",
    "assert round(bigram_model.score(\"out\", [\"called\"]), 2) == 0.47"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61aSnyFdgv_R"
   },
   "source": [
    "#### Prepare the data for the test set (sentences from ```test_sents```) and calculate the perplexity of both bigram and trigram models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "id": "7MOGp54lgJvL"
   },
   "outputs": [],
   "source": [
    "test_bigrams = generate_ngrams(test_sents, n=2)\n",
    "test_trigrams = generate_ngrams(test_sents, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veKOqphsiNOX"
   },
   "source": [
    "#### Note that for bigrams that did not appear in the training data, the perplexity of the model will be ```inf```. Here, your task is to:\n",
    "\n",
    "1.   Train a trigram model using the NLTK library\n",
    "2.   For the bigram and trigram models, compute the perplexity for each N-gram in the test set and average the values over the total number of Ngrams. Replace ```inf``` values with zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cct1eNGthk9N",
    "outputId": "d8e760a6-e7fd-4535-8916-f2d98e9d90c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2473"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 3\n",
    "\n",
    "train_data_2, train_chains_2 = padded_everygram_pipeline(n, train_sents_2)\n",
    "\n",
    "trigram_model = MLE(n)\n",
    "trigram_model.fit(train_data_2, train_chains_2)\n",
    "len(trigram_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "0.47\n"
     ]
    }
   ],
   "source": [
    "print(trigram_model.counts[\"called\"])\n",
    "print(round(trigram_model.score(\"out\", [\"called\"]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "6hD_vansjM-x"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "## perplexity calculation\n",
    "def compute_test_ngram_ppl(test_ngrams: list, model: MLE) -> float:\n",
    "    ppl = [model.perplexity(v) if not math.isinf(model.perplexity(v)) else 0 for v in test_ngrams]\n",
    "    return mean(ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "E9iFwFPtjdiQ"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "GRADING_ANSWER.Q8 = compute_test_ngram_ppl(test_bigrams, bigram_model)\n",
    "GRADING_ANSWER.Q9 = compute_test_ngram_ppl(test_trigrams, trigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6k2SfO39leC4"
   },
   "source": [
    "#### How do you think why the model achieves such good perplexity? Note that we computed the perplexity on N-grams only, but not on the sentences. We leave the sentence-level setting for an ungraded task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️Remember to **run the first code cell again** before submitting the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "at9hMZS8mU69"
   },
   "source": [
    "### Extra tasks. Ungraded.\n",
    "\n",
    "\n",
    "1.   Compare the models by computing the perplexity at the sentence-level. How often are the sentence probabilities zeroed out?\n",
    "2.   Apply a smoothing method to your count-based models. How does it influence the model perplexity?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week_3.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "ypKOdrGJTduSjnaxiT3baQ"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
